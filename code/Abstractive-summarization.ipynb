{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3c69fc0",
   "metadata": {},
   "source": [
    "# Import lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d93549",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers[sentencepiece] datasets sacrebleu rouge_score py7zr -q\n",
    "!pip install accelerate -U\n",
    "!pip install arabert\n",
    "!pip install datasets\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18512072",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import string\n",
    "from transformers import pipeline, set_seed\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from datasets import load_dataset, load_metric, Dataset\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "from transformers import TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7619ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e13c65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_any_word_in_text(substrings, text):\n",
    "    pattern = '|'.join([re.escape(substring) for substring in substrings])\n",
    "    match = re.search(pattern, text)\n",
    "    return bool(match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f48fa51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterDataframeByKeywords(df, column_name, search_keywords):\n",
    "    df['Match'] = df[column_name].apply(lambda x: is_any_word_in_text(search_keywords, x))\n",
    "    filtered_df = df[df['Match']]\n",
    "    filtered_df = filtered_df.drop([\"Match\"], axis=1).reset_index(drop=True)\n",
    "    return filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a32d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combineDataframes(dataframes):\n",
    "    combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "    combined_df.drop_duplicates(inplace=True)\n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edabb49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractHistoricalData(df, column_name):\n",
    "    pattern = r'\\b\\d{4}\\b'\n",
    "\n",
    "    filtered_df = df[df[column_name].str.contains(pattern, regex=True)]\n",
    "    return filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac00d248",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_record_with_words(df, col_name, keywords_list):\n",
    "    count = 0\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        text = row[col_name]\n",
    "        if any(keyword in text for keyword in search_keywords):\n",
    "            count += 1\n",
    "\n",
    "    print(f\"Number of records containing any of the search keywords: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a871b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_list = [\"الله\"\n",
    "                 ,\"محمد علي\"\n",
    "                 , \"الملك\"\n",
    "                 , \"مدينة\"\n",
    "                 , \"رومان\"\n",
    "                 , \"إمبراطور\"\n",
    "                 , \"شعب\"\n",
    "                 , \"بريطانيا\"\n",
    "                 , \"عبد الناصر\"\n",
    "                 ,\"القاهرة\"\n",
    "                 , \"أكتوبر\"\n",
    "                 ,\"فاروق\"\n",
    "                 , \"الحياة\"\n",
    "                 , \"الإسكندرية\"\n",
    "                 , \"القديم\"\n",
    "                 , \"عصر\"\n",
    "                 , \"مسلم\"\n",
    "                 ,  \"إسلام\"\n",
    "                 , \"العالم\"\n",
    "                 , \"عمر\"\n",
    "                 , \"الفرنسية\"\n",
    "                 , \"عرابي\"\n",
    "                 ,  'مصر'\n",
    "                 , 'تاريخ'\n",
    "                 , 'حضارة'\n",
    "                 , 'سيسي'\n",
    "                 , 'باشا'\n",
    "                 , 'ثورة'\n",
    "                 , 'عربي'\n",
    "                 , 'مصري'\n",
    "                 , 'مماليك'\n",
    "                 , 'دولة'\n",
    "                 , 'دين'\n",
    "                 , 'حرب'\n",
    "                 , 'جيش'\n",
    "                 , 'حكم'\n",
    "                 , 'بلاد'\n",
    "                 , \"رئيس\"\n",
    "                , \"العراق\"\n",
    "                , \"فتح\"\n",
    "                , \"السلطان\"\n",
    "                , \"بابل\"\n",
    "                , \"قوات\"\n",
    "                , \"رومان\"\n",
    "                , \"فرنسا\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666e4f2c",
   "metadata": {},
   "source": [
    "# Working on xlsum dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689c6d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_xlsum = load_dataset(\"csebuetnlp/xlsum\", \"arabic\")\n",
    "\n",
    "split_lengths = [len(dataset_xlsum[split])for split in dataset_xlsum]\n",
    "\n",
    "print(f\"Split lengths: {split_lengths}\")\n",
    "print(f\"Features: {dataset_xlsum['train'].column_names}\")\n",
    "print(\"\\nText:\")\n",
    "\n",
    "print(dataset_xlsum[\"test\"][1][\"text\"])\n",
    "\n",
    "print(\"\\nSummary:\")\n",
    "\n",
    "print(dataset_xlsum[\"test\"][1][\"summary\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe0e364",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_xlsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96116647",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import concatenate_datasets\n",
    "\n",
    "\n",
    "merged_xlsum_dataset = concatenate_datasets([dataset_xlsum[\"train\"], dataset_xlsum[\"validation\"], dataset_xlsum[\"test\"]])\n",
    "\n",
    "df_xlsum_merged = merged_xlsum_dataset.to_pandas()\n",
    "\n",
    "print(df_xlsum_merged.shape)\n",
    "print(df_xlsum_merged.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7ba9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_xlsum_merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f81607",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_xlsum_merged = df_xlsum_merged.drop([\"title\", \"url\", \"id\"], axis=1)\n",
    "df_xlsum_filtered = filterDataframeByKeywords(df_xlsum_merged, \"summary\", keywords_list)\n",
    "df_xlsum_filtered['summary'], df_xlsum_filtered[\"text\"] = df_xlsum_filtered[\"text\"], df_xlsum_filtered['summary']\n",
    "df_xlsum_filtered.rename(columns={'summary': 'text', \"text\": \"summary\"}, inplace=True)\n",
    "df_xlsum_merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43cc04a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_xlsum_filtered.shape)\n",
    "for i in range(5):\n",
    "    print(f\"summary {i}: \", df_xlsum_filtered.loc[i, \"summary\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685d5353",
   "metadata": {},
   "source": [
    "# Working on labeld_validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68a2cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = pd.read_json('/content/labeled_validation_dataset.jsonl', lines=True).drop([\"example_id\"], axis=1)\n",
    "val_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353786d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset.rename(columns={'paragraph': 'text'}, inplace=True)\n",
    "val_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f78c0a",
   "metadata": {},
   "source": [
    "# Concatenate the xlsum dataset and the validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e5a1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the data frames vertically (along rows)\n",
    "concatenated_df = pd.concat([df_xlsum_filtered, val_dataset])\n",
    "\n",
    "# Reset the index of the concatenated data frame\n",
    "concatenated_df = concatenated_df.reset_index(drop=True)\n",
    "concatenated_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55376725",
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf6ed5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    print(f\"summary {i}: \", concatenated_df.loc[i, \"summary\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abcc345",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenated_df.to_csv('xlsum_val_concatenated.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2a194c",
   "metadata": {},
   "source": [
    "# Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be309518",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "from arabert.preprocess import ArabertPreprocessor\n",
    "\n",
    "model_name=\"abdalrahmanshahrour/arabartsummarization\"\n",
    "preprocessor = ArabertPreprocessor(model_name=\"\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)\n",
    "pipeline = pipeline(\"text2text-generation\",model=model,tokenizer=tokenizer)\n",
    "\n",
    "# text = \"\"\"وتحت عنوان من الكارثة إلى التحدى يبدأ الكاتب عرض الكتاب الرابع ، حيث يوضح كيف كانت إسرائيل فرحة بنصرها عام 67 وأنها ارتاحت لاعتقادها بأن هناك وقتا طويلا وطويلا جدا قبل أن يفيق العرب من صدمة 67، وكيف أن القوات الجوية للجمهورية العربية المتحدة قد فاجأتها بعد شهر واحد من نهاية حرب 67 بهجوم جوى عنيف على مواقعها فى سيناء وكان هذا إعلانا عن بداية حرب من نوع جديد هى حرب الاستنزاف التى استمرت حتى تم وقف إطلاق النار بين الطرفين فى 8 أغسطس 1970، ثم وفاة عبدالناصر وتولى أنور السادات حكم مصر واستعداده للحرب . ويتعرض الكاتب أيضا وبصورة سريعة لفلسطين والأردن وسوريا قبل أن ينتقل إلى الكتاب الخامس عن حرب أكتوبر ، حيث يعرض الخطط والاستعدادات المصرية ثم الاستعدادات الإسرائيلية ثم يبدأ بعرض وقائع الحرب بداية من الضربة الجوية وانهيار خط بارليف واختراقه ، ويتوقف الكاتب عند يوم 8 أكتوبر ، ويقول : إن هذا اليوم كان اسوأ هزيمة فى تاريخ الجيش الإسرائيلى ثم ينتقل بنا المؤلف إلى الجبهة السورية ثم يعود ثانية إلى يوميات الحرب حتى 7 9 أكتوبر إلى 9 13 أكتوبر ثم 14 أكتوبر ، ثم يعرض للثغرة أو ما عرف بعملية المزرعة الصينية يوم 16 و 15 أكتوبر والمساعدات الأمريكية الضخمة لإسرائيل ، ثم بداية الضغوط السياسية على الرئيس أنور السادات من 17 19 أكتوبر ثم ينتقل الكاتب للأحداث التى جرت من 17 20 أكتوبر وإعفاء الفريق الشاذلى من منصبه كرئيس لأركان القوات المسلحة المصرية ، وتولى الفريق الجمسى بدلا منه ثم الاتجاه إلى الموافقة على طلب وقف إطلاق النار والخلاف مع سوريا بشأن هذا الأمر ، ثم بداية الهجوم الإسرائيلى من 19 إلى 22 أكتوبر على الضفة الغربية لقناة السويس والعمليات النهائية فى سوريا 14 23 أكتوبر ، وكيف أن الملك حسين قرر دخول الحرب ضد إسرائيل يوم 9 أكتوبر ، ثم يعرض الكاتب المعركة الخاصة بالاستيلاء على مدينة السويس من 23 أكتوبر إلى 25 أكتوبر ثم تطورات هذه المعركة ، وكيف أنه مع حلول يوم السابع والعشرين من أكتوبر كان الإسرائيليون قد أسروا نحو ثمانية آلاف فرد من القوات المصرية ، أغلبهم من وحدات الإمداد والتموين\"\"\"\n",
    "# text = preprocessor.preprocess(text)\n",
    "\n",
    "# result = pipeline(text,\n",
    "#             pad_token_id=tokenizer.eos_token_id,\n",
    "#             num_beams=3,\n",
    "#             repetition_penalty=3.0,\n",
    "#             max_length=200,\n",
    "#             length_penalty=1.0,\n",
    "#             no_repeat_ngram_size = 3)[0]['generated_text']\n",
    "# result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c6eea6",
   "metadata": {},
   "source": [
    "# Prepare data for finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fefee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset_from_dataframe(df, train_ratio= 0.8, val_ratio = 0.1, test_ratio = 0.1):\n",
    "    # Shuffle the DataFrame\n",
    "    df = shuffle(df, random_state=42)\n",
    "\n",
    "    # Split the DataFrame into train, validation, and test sets\n",
    "    train_size = int(train_ratio * len(df))\n",
    "    val_size = int(val_ratio * len(df))\n",
    "    test_size = len(df) - train_size - val_size\n",
    "\n",
    "\n",
    "    train_df = df[:train_size]\n",
    "    val_df = df[train_size : train_size + val_size]\n",
    "    test_df = df[-test_size:]\n",
    "\n",
    "    # Convert the train, validation, and test DataFrames to datasets.arrow_dataset.Dataset\n",
    "    train_dataset = Dataset.from_pandas(train_df)\n",
    "    val_dataset = Dataset.from_pandas(val_df)\n",
    "    test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "    return train_dataset, val_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a381526a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, val_dataset, test_dataset = make_dataset_from_dataframe(concatenated_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82bc8e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_dataset)\n",
    "print(val_dataset)\n",
    "print(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d341e9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.remove_columns('__index_level_0__')\n",
    "val_dataset = val_dataset.remove_columns('__index_level_0__')\n",
    "test_dataset = test_dataset.remove_columns('__index_level_0__')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7518a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nText:\")\n",
    "\n",
    "print(train_dataset[\"text\"][0])\n",
    "\n",
    "print(\"\\nSummary:\")\n",
    "\n",
    "print(train_dataset[\"summary\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87da0e62",
   "metadata": {},
   "source": [
    "# visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272e4b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_token_len = len([tokenizer.encode(s) for s in train_dataset[\"text\"]])\n",
    "\n",
    "summary_token_len = len([tokenizer.encode(s) for s in train_dataset[\"summary\"]])\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "axes[0].hist(text_token_len, bins = 20, color = 'C0', edgecolor = 'C0' )\n",
    "axes[0].set_title(\"Text Token Length\")\n",
    "axes[0].set_xlabel(\"Length\")\n",
    "axes[0].set_ylabel(\"Count\")\n",
    "\n",
    "axes[1].hist(summary_token_len, bins = 20, color = 'C0', edgecolor = 'C0' )\n",
    "axes[1].set_title(\"Summary Token Length\")\n",
    "axes[1].set_xlabel(\"Length\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b035ebc",
   "metadata": {},
   "source": [
    "# convert_examples_to_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6594a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_examples_to_features(example_batch):\n",
    "    # max_length 1024 or 512\n",
    "    input_encodings = tokenizer(example_batch['text'] , max_length = 512, truncation = True, padding=\"max_length\", return_tensors=\"pt\" )\n",
    "\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        target_encodings = tokenizer(example_batch['summary'], max_length = 128, truncation = True )\n",
    "\n",
    "    return {\n",
    "        'input_ids' : input_encodings['input_ids'],\n",
    "        'attention_mask': input_encodings['attention_mask'],\n",
    "        'labels': target_encodings['input_ids']\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd4fa19",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_pt = train_dataset.map(convert_examples_to_features, batched = True)\n",
    "test_dataset_pt = test_dataset.map(convert_examples_to_features, batched = True)\n",
    "val_dataset_pt = val_dataset.map(convert_examples_to_features, batched = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79eb8a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_dataset_pt)\n",
    "print(\"\\nText:\")\n",
    "print(train_dataset_pt[\"text\"][0])\n",
    "\n",
    "print(\"\\nSummary:\")\n",
    "print(train_dataset_pt[\"summary\"][0])\n",
    "\n",
    "print(\"\\nInput_ids:\")\n",
    "print(train_dataset_pt[\"input_ids\"][0])\n",
    "\n",
    "print(\"\\nAttention_mask:\")\n",
    "print(train_dataset_pt[\"attention_mask\"][0])\n",
    "\n",
    "print(\"\\nLabels:\")\n",
    "print(train_dataset_pt[\"labels\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50fbe4a7",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9008406f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "seq2seq_data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e7cea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "     \n",
    "%cd /content/drive/MyDrive/ICMTC Competition/Model/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b71f84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "trainer_args = TrainingArguments(\n",
    "    output_dir='arabert-xlsum', num_train_epochs=20, warmup_steps=500,\n",
    "    per_device_train_batch_size=8, per_device_eval_batch_size=8,\n",
    "    weight_decay=0.1, logging_steps=10,\n",
    "    evaluation_strategy='steps', eval_steps=500, save_steps=1e6,\n",
    "    gradient_accumulation_steps=16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7fd630",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model=model, args=trainer_args,\n",
    "                  tokenizer=tokenizer, data_collator=seq2seq_data_collator,\n",
    "                  train_dataset=train_dataset_pt,\n",
    "                  eval_dataset=val_dataset_pt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d23af3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50bc7e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save model\n",
    "model.save_pretrained(\"arabert-xlsum-model\")\n",
    "\n",
    "# Save tokenizer\n",
    "tokenizer.save_pretrained(\"tokenizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42cd3ecc",
   "metadata": {},
   "source": [
    "# Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316b5aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch_sized_chunks(list_of_elements, batch_size):\n",
    "    \"\"\"split the dataset into smaller batches that we can process simultaneously\n",
    "    Yield successive batch-sized chunks from list_of_elements.\"\"\"\n",
    "    for i in range(0, len(list_of_elements), batch_size):\n",
    "        yield list_of_elements[i : i + batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc18667c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def generate_batch_sized_chunks(list_of_elements, batch_size):\n",
    "    \"\"\"split the dataset into smaller batches that we can process simultaneously\n",
    "    Yield successive batch-sized chunks from list_of_elements.\"\"\"\n",
    "    for i in range(0, len(list_of_elements), batch_size):\n",
    "        yield list_of_elements[i : i + batch_size]\n",
    "\n",
    "\n",
    "\n",
    "def calculate_metric_on_test_ds(dataset, metric, model, tokenizer,\n",
    "                               batch_size=16, device=device,\n",
    "                               column_text=\"article\",\n",
    "                               column_summary=\"highlights\"):\n",
    "    article_batches = list(generate_batch_sized_chunks(dataset[column_text], batch_size))\n",
    "    target_batches = list(generate_batch_sized_chunks(dataset[column_summary], batch_size))\n",
    "\n",
    "    for article_batch, target_batch in tqdm(\n",
    "        zip(article_batches, target_batches), total=len(article_batches)):\n",
    "\n",
    "        inputs = tokenizer(article_batch, max_length=1024,  truncation=True,\n",
    "                        padding=\"max_length\", return_tensors=\"pt\")\n",
    "\n",
    "        summaries = model.generate(input_ids=inputs[\"input_ids\"].to(device),\n",
    "                         attention_mask=inputs[\"attention_mask\"].to(device),\n",
    "                         length_penalty=0.8, num_beams=8, max_length=128)\n",
    "        ''' parameter for length penalty ensures that the model does not generate sequences that are too long. '''\n",
    "\n",
    "        # Finally, we decode the generated texts,\n",
    "        # replace the  token, and add the decoded texts with the references to the metric.\n",
    "        decoded_summaries = [tokenizer.decode(s, skip_special_tokens=True,\n",
    "                                clean_up_tokenization_spaces=True)\n",
    "               for s in summaries]\n",
    "\n",
    "        decoded_summaries = [d.replace(\"\", \" \") for d in decoded_summaries]\n",
    "\n",
    "\n",
    "        metric.add_batch(predictions=decoded_summaries, references=target_batch)\n",
    "\n",
    "    #  Finally compute and return the ROUGE scores.\n",
    "    score = metric.compute()\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06985ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge_names = [\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"]\n",
    "\n",
    "rouge_metric = load_metric('rouge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa83076",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = calculate_metric_on_test_ds(\n",
    "    test_dataset_pt, rouge_metric, trainer.model, tokenizer, batch_size = 8, column_text = 'text', column_summary= 'summary'\n",
    ")\n",
    "\n",
    "rouge_dict = dict((rn, score[rn].mid.fmeasure ) for rn in rouge_names )\n",
    "\n",
    "pd.DataFrame(rouge_dict, index = [f'arabert'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f797c2d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2252c7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9932cb4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca32db76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
